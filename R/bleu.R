library(checkmate)
library(tokenizers)

#' Compute BLEU Score
#'
#' Finds effective reference length, which is necessary to compute the
#' brevity penalty, that is the closest length to the candidate's length.
#'
#' @param cand_len Length of the candidate sentence.
#' @param reference List of reference sentences.
#' @return Effective reference length.
eff_ref_len <- function(cand_len, reference) {
  # TODO Test
  ref_lengths <- vapply(reference, length, numeric(1))
  ref_length_ind <- which.min(abs(ref_lengths - cand_len))
  return(ref_lengths[[ref_length_ind]])
}

#' Computes Brevity Penalty
#'
#' TODO: Explain brevity penalty.
#'
#' @param cand_len Length of the candidate sentence.
#' @param eff_ref_len Effective reference length.
#' @return Brevity Penalty.
brevity_pen <- function(cand_len, eff_ref_len) {
  # TODO: Check necessity
  if (eff_ref_len == 0) {
    return(0)
  }
  if (cand_len > eff_ref_len) {
    return(1)
  } else {
    return(exp(1 - (eff_ref_len / cand_len)))
  }
}

brev_pen <- function(candidate, reference) {
  # TODO check cand and ref for same length
  combined <- data.frame(cand = candidate)
  combined$ref <- reference
  combined$eff_len <- apply(
    combined,
    1,
    function(e) {

    }
  )
}

#' Get n-gram Frequency Table
#'
#' Computes frequency table for n-grams.
#'
#' @param reference List of reference sentences.
#' @param n Determining n-length of n-gram.
#' @return Frequency table.s
ref_n_gram_count <- function(reference, n = 2) {
  # TODO: Pipe
  n_grams_list <- tokenizers::tokenize_ngrams(reference, n = n)
  n_gram_tables <- lapply(n_grams_list, table)
  n_gram_df <- lapply(n_gram_tables, as.data.frame)
  res <- Reduce(function(...) merge(..., all = TRUE, by = "Var1"), n_gram_df)
  res$max_ref <- apply(res[-1], 1, max, na.rm = TRUE)
  return(res[c("Var1", "max_ref")])
}

#' Compute Modified Precision
#'
#' Function to compute modified precision. Sum over all n-grams in which the
#' numerator includes the total number of n-grams from the produced candidate
#' sentence (clipped at max of the n-gram count from the reference to avoid
#' larger values than 1). Denominator includes counts of n-grams from reference
#' sentence.
#'
#' @param candidate A candidate sentence (e.g. generated by a model).
#' @param reference List of reference sentences.
#' @param n n for n-gram
#' @return Modified precision for candidate and reference
mod_precision <- function(candidate, reference, n = 2) {

  candidate <- tolower(candidate)
  reference <- tolower(reference)

  cand_n_grams <- tokenizers::tokenize_ngrams(
    candidate,
    n = n)
  cand_occurences <- as.data.frame(
    table(cand_n_grams))
  ref_occurences <- ref_n_gram_count(
    reference = reference,
    n = n)

  # Get only n-grams that occur in the candidate
  combined <- merge(
    ref_occurences,
    cand_occurences,
    by.x = "Var1",
    by.y = "cand_n_grams",
    all = TRUE)
  combined[is.na(combined)] <- 0

  # Get clipped values
  combined$clipped <- apply(
    combined,
    1,
    function(e) {
      ref_count <- as.numeric(unname(e[2]))
      cand_count <- as.numeric(unname(e[3]))
      if (ref_count > cand_count) return(cand_count) else return(ref_count)
      }
    )

  # Compute modified precision
  # Freq is frequency of n-grams in candidate
  denominator <- sum(combined$Freq)
  nominator <- sum(combined$clipped)
  # if (denominator == 0) return(0) else
  return(nominator / denominator)
}

#' Compute BLEU Score
#'
#' This function computes BLEU (Bilingual Evaluation Understudy) according
#' to the paper of Papineni et al. 2002. TODO: Write Comprehensive
#' Documentation.
#' TODO: Write examples
#'
#' @param candidate A candidate sentence (e.g. generated by a model).
#' @param reference List of reference sentences.
#' @param weights List of weights for each n (n-gram). Must be of size N.
#' @param N Number n of maximum n-grams.
#' @return BLEU-Score for candidate and reference sentences.
bleu <- function(candidate, reference, weights = c(0.25, 0.25, 0.25, 0.25), N = 4) {
  # TODO: Check correct types
  # TODO: Check dimension for vectorized
  # TODO: Create weight vector automatically
  # checkmate::checkAtomic(candidate) vector of strings c(string)
  # checkmate::checkArray(reference) vector of list of strings list(c(string))
  # checkmate::checkEqual(length(cand), length(reference))
  cand_len <- length(candidate)
  erl <- eff_ref_len(cand_len, reference)
  bp <- brevity_pen(cand_len, erl)
  mod_precision_list <- unlist(lapply(
    seq(N),
    function(n_len) {
      return(mod_precision(
        candidate = candidate,
        reference = reference,
        n = n_len))
    }))
  print(weights)
  print(mod_precision_list)
  return((bp * exp(weights %*% log(mod_precision_list)))[1])
}

bleu_list <- function(cand_list, ref_list, weights = c(0.25, 0.25, 0.25, 0.25), N = 4) {

}

bleu_dataframe <- function(cand_ref_df, weights = c(0.25, 0.25, 0.25, 0.25), N = 4) {
  # TODO
}

bleu_corpus <- function(cand_path, ref_path, weights = c(0.25, 0.25, 0.25, 0.25), N = 4) {
  # TODO
}
